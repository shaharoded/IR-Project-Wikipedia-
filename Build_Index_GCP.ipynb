{"cells":[{"cell_type":"markdown","id":"8bd4332c","metadata":{"id":"8bd4332c"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"0e73e299","metadata":{"id":"0e73e299","outputId":"e67c19a1-0872-487d-fa31-413107af9824"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-54e1  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable\n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1\n","# !gcloud dataproc clusters list --region us-south1"]},{"cell_type":"code","execution_count":2,"id":"44875ca8","metadata":{"id":"44875ca8","outputId":"bc1f61a7-30d6-4aa5-b8b5-80a03c0ed4a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"4924c5ce","metadata":{"id":"4924c5ce","outputId":"bf34c6f8-21e8-4ff7-eb74-6ae512e5ebc8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# PySpark:\n","import pyspark\n","\n","# Standard library imports\n","import hashlib\n","import itertools\n","import os\n","import pickle\n","import re\n","import string\n","from dateutil import parser\n","import math\n","from math import log\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","from itertools import islice, count, groupby\n","from operator import itemgetter\n","from pathlib import Path\n","from time import time\n","\n","# Third-party imports\n","import pandas as pd\n","from google.cloud import storage\n","import numpy as np\n","\n","# NLTK downloads (consider moving these to a setup script or documentation on required corpora)\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.data.path.append('/root/nltk_data')\n","\n","# Function definition\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()"]},{"cell_type":"code","execution_count":4,"id":"e9191f54","metadata":{"id":"e9191f54","outputId":"1c61a31e-74b1-4de5-9092-cbedcce9b507"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar 10 12:08 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"c8f95ce4","metadata":{"id":"c8f95ce4"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"9b21d2a8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Executor Memory: 2893m\n","Driver Memory: 2048m\n","Executor Memory Overhead: None\n","Driver Memory Overhead: None\n","Driver Max Result Size: 1024m\n"]}],"source":["# Print executor and driver memory settings\n","print(\"Executor Memory:\", spark.sparkContext.getConf().get(\"spark.executor.memory\"))\n","print(\"Driver Memory:\", spark.sparkContext.getConf().get(\"spark.driver.memory\"))\n","print(\"Executor Memory Overhead:\", spark.sparkContext.getConf().get(\"spark.executor.memoryOverhead\"))\n","print(\"Driver Memory Overhead:\", spark.sparkContext.getConf().get(\"spark.driver.memoryOverhead\"))\n","print(\"Driver Max Result Size:\", spark.sparkContext.getConf().get(\"spark.driver.maxResultSize\"))"]},{"cell_type":"code","execution_count":6,"id":"84431a3c","metadata":{"id":"84431a3c","outputId":"070967c0-ea7a-419b-9549-454624c26f48"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-54e1-m.c.ir-assignment-3-413716.internal:37883\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fb939b9f280>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"8c261c4f","metadata":{"id":"8c261c4f"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'shahar_ir_project'\n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'init_file.sh' and b.name != 'graphframes.sh' and not b.name.startswith(\"PostingList_\") and not b.name.startswith(\"BigDataFiles\") and not b.name.startswith(\"Index\"):\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"86ecf5c7","metadata":{"id":"86ecf5c7"},"source":["# Building an inverted index (Test, small)"]},{"cell_type":"code","execution_count":8,"id":"d73b085a","metadata":{"id":"d73b085a","outputId":"f473ad34-b480-462c-b9d2-be051d2a8d03"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# paths = paths[:5]\n","parquetFile = spark.read.parquet(*paths)"]},{"cell_type":"code","execution_count":9,"id":"4ac6b1e4","metadata":{"id":"4ac6b1e4","outputId":"5eb8e973-845a-42b1-b3ed-525c6114fc45"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Get N - number of documents in indices\n","N = parquetFile.count()\n","N"]},{"cell_type":"code","execution_count":10,"id":"3fea2c8c","metadata":{"id":"3fea2c8c","outputId":"2e654fbf-2b04-4e60-ef54-c6f3a192b86f"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":11,"id":"325912e9","metadata":{"id":"325912e9"},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":12,"id":"22c5eabe","metadata":{"id":"22c5eabe"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"f1eb3217","metadata":{},"source":["**Extracting RDDs from the source files**\n"]},{"cell_type":"code","execution_count":13,"id":"67bff0e1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_text_pairs_title = parquetFile.select(\"title\", \"id\").rdd\n","doc_text_pairs_body = parquetFile.select(\"text\", \"id\").rdd\n","\n","# Create {ID:Title} dictionary using spark for retrieval on query\n","id_title_rdd = doc_text_pairs_title"]},{"cell_type":"markdown","id":"112d45a1","metadata":{},"source":["**Parsing Functions**"]},{"cell_type":"code","execution_count":14,"id":"425b7599","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\n","    'category', 'references', 'also', 'links', 'extenal', 'see',\"links\",\n","                    \"may\", \"first\",\"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\", '.', ',', '?', '!', ':', ';', '/', '\\\\', '-', '\"', \"'\", \"(\", \")\",\n","    \"[\", \"]\", \"{\", \"}\", \"|\", \"*\", \"+\", \"@\", \"^\", \"&\", \"%\", \"#\", \"''\",'``','...', '',' ', None\n","]\n","corpus_stopwords = set(corpus_stopwords + list(string.punctuation))\n","\n","ALL_STOPWORDS = english_stopwords.union(corpus_stopwords)\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","def replace_contractions(token):\n","    \"\"\"\n","    Replace common contractions like \"n't\" with their full words.\n","    \n","    Parameters:\n","        tokens (list of str): List of tokens.\n","        \n","    Returns:\n","        list of str: List of tokens with contractions replaced.\n","    \"\"\"\n","    contractions_mapping = {\n","        \"n't\": \"not\",\n","        \"'nt\": \"not\",\n","        \"'ll\": \"will\",\n","        \"'ve\": \"have\",\n","        \"'re\": \"are\",\n","        \"'d\": \"would\",\n","        \"'m\": \"am\",\n","        \".\": \"\",\n","        \"'s\": \"is\"\n","    }\n","    \n","    for key in contractions_mapping.keys():\n","        if key in token:\n","            i = token.index(key)\n","            if key == \".\":\n","                return [token[:i], token[i+1:]]\n","            return [token[:i], contractions_mapping[key]]\n","    return [token]\n","\n","\n","def ParseDateFromToken(token):\n","    def is_likely_date(token):\n","        # Check for delimiters typically found in numeric shaped dates\n","        if re.search(r'(\\d{1,4}[- /.]\\d{1,2}[- /.]\\d{1,4})', token):\n","            return True\n","        return False\n","\n","    if not is_likely_date(token):\n","        return None\n","\n","    try:\n","        parsed_date = parser.parse(token, ignoretz=True)\n","        \n","        # Validate year range to prevent OverflowError\n","        if parsed_date.year < 1 or parsed_date.year > 9999:\n","            return None\n","\n","        # Further validation to ignore times without explicit dates\n","        if parsed_date.hour != 0 or parsed_date.minute != 0 or parsed_date.second != 0:\n","            # This means the token was more like a time than a date\n","            return None\n","\n","        year = parsed_date.year\n","        full_month_name = parsed_date.strftime(\"%B\")\n","        return str(year), full_month_name\n","    except (ValueError, OverflowError, TypeError):\n","        return None\n","\n","def PreProcessText(text):\n","    '''\n","    Parse input text. Titles - all lowered, body - keeps capitalization under conditions.\n","    Out is a final list of lemmatized tokens.\n","    '''\n","    in_tokens = word_tokenize(text)\n","    in_tokens = [token.strip(string.punctuation) for token in in_tokens]\n","\n","    tok_tmp = []\n","    for token in in_tokens:\n","      tmp = replace_contractions(token)\n","      tok_tmp.extend(tmp)\n","   \n","    tokens = [token.lower() for token in tok_tmp if token not in ALL_STOPWORDS]\n","    #     tok_tmp = []\n","    #     for token in tokens:\n","    #         date_data = ParseDateFromToken(token)  # list of 2 or None\n","    #         if date_data is not None:\n","    #             tok_tmp.extend(date_data)\n","\n","    #     tokens.extend(tok_tmp)\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    return tokens"]},{"cell_type":"markdown","id":"8ddd7047","metadata":{},"source":["**Parsing Text**"]},{"cell_type":"code","execution_count":15,"id":"b56378b9","metadata":{},"outputs":[],"source":["def ParseDocumentToPosting(text, id):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in\n","    `ALL_STOPWORDS` and return entries that will go into our posting lists.\n","    This is the basis of the posting list - creating a posting list for 1 document with basic TF.\n","    Build enviroment parameter and term vocabulaty in the process.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    # Preprocess the entire test\n","    tokens = PreProcessText(text)\n","\n","    # Count the term frequency of each token\n","    tf = Counter(tokens)\n","\n","    # Create the list of tuples (token, (doc_id, tf) - will be replaced down the line\n","    result = [(token, (id, freq)) for token, freq in tf.items()]\n","    return result\n","\n","\n","def ParseDocumentToDocSize(text, id):\n","    ''' Get the doc size of parsed tokens in the document that is not included in\n","    `ALL_STOPWORDS` and return entries that will go into our Index.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    InvertedIndexType: str\n","      Type of index ['title', 'body']\n","    Returns:\n","    --------\n","    Tuple (for RDD)\n","      (doc_id: len(tokens))\n","    '''\n","    # Preprocess the entire test\n","    tokens = PreProcessText(text)\n","    n_terms = len(tokens)\n","\n","    # Return DocSize dict {doc_id: len(tokens)}\n","\n","    return (id, n_terms)"]},{"cell_type":"markdown","id":"1333f1cf","metadata":{},"source":["**Reduce Output to a Posting List, Keep Significant Keys**"]},{"cell_type":"code","execution_count":16,"id":"f08432f3","metadata":{},"outputs":[],"source":["def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","  return sorted_pl"]},{"cell_type":"markdown","id":"0ac8743b","metadata":{},"source":["**Calculate DF (W2DF) Object and Term Index**"]},{"cell_type":"code","execution_count":17,"id":"2488e7e8","metadata":{},"outputs":[],"source":["from math import log\n","\n","def TermStats(postings, N):\n","    ''' Takes a posting list RDD and calculate the df and idf for each token.\n","    Parameters:\n","    -----------\n","      postings: RDD\n","        An RDD where each element is a (token, posting_list) pair.\n","      N: int\n","        Total number of documents in the corpus.\n","    Returns:\n","    --------\n","      Dictionary1\n","        An Dict where each element is a (token, (unique_index, idf)) pair. index si unique across all corpus vocab\n","      Dictionary2\n","        sub Dict of term: DF (df_rdd) for reading from the Index, not needed for methodological retrieval object\n","    '''\n","    df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n","    idf_rdd = df_rdd.map(lambda x: (x[0], log(N / (x[1] + 1), 10)))  # Using log base 10 for IDF calculation\n","\n","    # Assuming the rest of your original processing here...\n","\n","    # Convert IDF RDD to a dictionary for easier index assignment\n","    idf_dict = idf_rdd.collectAsMap()\n","\n","    # Assign an index to each term based on its position in the dictionary\n","    term_idf_mapping = {term: (index, idf) for index, (term, idf) in enumerate(idf_dict.items())}\n","\n","    return term_idf_mapping, df_rdd.collectAsMap()"]},{"cell_type":"markdown","id":"b9300d2b","metadata":{},"source":["**Page Rank**"]},{"cell_type":"code","execution_count":18,"id":"1ca49c28","metadata":{},"outputs":[],"source":["def generate_graph(pages):\n","    ''' Compute the directed graph generated by wiki links.\n","    Parameters:\n","    -----------\n","    pages: RDD\n","        An RDD where each row consists of one Wikipedia article with 'id' and\n","        'anchor_text'.\n","    Returns:\n","    --------\n","    edges: RDD\n","        An RDD where each row represents an edge in the directed graph created by\n","        the Wikipedia links. The first entry should be the source page id and the\n","        second entry is the destination page id. No duplicates should be present.\n","    vertices: RDD\n","        An RDD where each row represents a vertex (node) in the directed graph\n","        created by the Wikipedia links. No duplicates should be present.\n","    '''\n","    # Generate edges RDD using map and reduceByKey\n","    edges = pages.flatMap(lambda page: [(page.id, anchor.id) for anchor in page.anchor_text])\n","    edges = edges.map(lambda edge: (edge, None)).reduceByKey(lambda x, _: x).keys()\n","\n","    # Generate vertices RDD using flatMap and distinct\n","    vertices = edges.flatMap(lambda edge: [(edge[0],), (edge[1],)]).distinct()\n","\n","    return edges, vertices"]},{"cell_type":"markdown","id":"1537a4eb","metadata":{},"source":["**Create Additional Info Component to Indices**"]},{"cell_type":"code","execution_count":19,"id":"cbd4b274","metadata":{},"outputs":[],"source":["def AdditionalInfoDict(doc_text_pairs_title, doc_sizes):\n","    '''\n","    Creates a dictionary with document ID as keys and tuples of (title, doc_size) as values.\n","\n","    Parameters:\n","    -----------\n","    doc_text_pairs_title: RDD\n","        An RDD of tuples (title, doc_id).\n","    doc_sizes: RDD\n","        An RDD of tuples (doc_id, doc_size).\n","\n","    Returns:\n","    --------\n","    dict\n","        A dictionary where each key is a doc_id and each value is a tuple (title, doc_size).\n","    '''\n","\n","    # Convert doc_text_pairs_title to have doc_id as key\n","    doc_id_with_title = doc_text_pairs_title.map(lambda x: (x[1], x[0]))  # (doc_id, title)\n","\n","    # Join doc_id_with_title RDD with doc_sizes RDD based on doc_id\n","    joined_rdd = doc_id_with_title.join(doc_sizes)  # ((doc_id, (title, doc_size)))\n","\n","    # Transform the joined RDD into the desired dictionary format\n","    result_dict = joined_rdd.map(lambda x: (x[0], (x[1][0], x[1][1]))).collectAsMap()\n","\n","    return result_dict"]},{"cell_type":"markdown","id":"72b1fb3f","metadata":{},"source":["**Create Doc TF-IDF Sparse Vector**"]},{"cell_type":"code","execution_count":20,"id":"0868f7be","metadata":{},"outputs":[],"source":["def CalculateDocTFIDF_Vectors(postings_rdd, term_idf_mapping, doc_additional_info):\n","    '''\n","    This function is desined to create a document sparse vector, filled with TF-IDF values, \n","    used in order to calculate CosineSimilarity. In reality, this method was used for the title index only.\n","    '''\n","    # Broadcast the term_idf_mapping and doc_additional_info to all workers\n","    term_idf_mapping_bc = sc.broadcast(term_idf_mapping)\n","    doc_additional_info_bc = sc.broadcast(doc_additional_info)\n","\n","    # Calculate TF-IDF scores, including normalization by document size\n","    # Attempting repartition to allow fixed size partitions to be processed\n","    tfidf_normalized_rdd = postings_rdd.repartition(1000).flatMap(lambda x: [\n","        ((doc_id, term_idf_mapping_bc.value[x[0]][0]),  # Use term to get (index, idf) and select index\n","         (tf * term_idf_mapping_bc.value[x[0]][1]) / doc_additional_info_bc.value[doc_id][1])  # Multiply TF by IDF and divide by doc_size\n","        for doc_id, tf in x[1]  # x[1] is a list of tuples (doc_id, not_norm_tf)\n","        if x[0] in term_idf_mapping_bc.value  # Check if the term is in the term_idf_mapping\n","    ]).reduceByKey(lambda x, y: x + y)\n","\n","    # Convert the RDD to a format of (doc_id, {term_index: normalized_tfidf_score, ...})\n","    doc_normalized_tfidf_vectors_rdd = tfidf_normalized_rdd.map(lambda x: (x[0][0], (x[0][1], x[1])))\\\n","                                                            .groupByKey()\\\n","                                                            .mapValues(lambda x: dict(x))  # Convert Iterable to Dict for each doc_id\n","\n","    return doc_normalized_tfidf_vectors_rdd.collectAsMap()"]},{"cell_type":"markdown","id":"d4f74097","metadata":{},"source":["**Write Indices to Bucket**"]},{"cell_type":"code","execution_count":24,"id":"eb4a9b07","metadata":{},"outputs":[],"source":["NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings, BASE_DIR, bucket_name = bucket_name):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # YOUR CODE HERE\n","    # Step 1: Map postings to buckets\n","    postings = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","\n","    # Step 2: Group postings by bucket\n","    postings_grouped = postings.groupByKey().mapValues(list)\n","\n","    # Step 3: Write postings for each bucket to disk and collect location info\n","    def write_bucket_postings(bucket_postings):\n","      bucket_id, postings_list = bucket_postings\n","      # Prepare the argument for write_a_posting_list\n","      # This single argument is a tuple containing the bucket ID and the list of postings\n","      b_w_pl = (bucket_id, postings_list)\n","      # Call write_a_posting_list with the correctly formatted argument\n","      posting_locs = InvertedIndex.write_a_posting_list(b_w_pl, BASE_DIR, bucket_name)\n","      return posting_locs\n","\n","    # Step 4: Apply write_bucket_postings to each group of postings\n","    posting_locations_rdd = postings_grouped.map(write_bucket_postings)\n","\n","    return posting_locations_rdd"]},{"cell_type":"markdown","id":"c410eb26","metadata":{},"source":["**Build PageViews (Built once, used externally)**"]},{"cell_type":"code","execution_count":25,"id":"061ea6ac","metadata":{},"outputs":[],"source":["# Using user page views (as opposed to spiders and automated traffic) for the\n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path)\n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","# Download the file (2.3GB)\n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly\n","# total number of page views (5). Then, remove lines with article id or page\n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same\n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","  for line in f:\n","    parts = line.split(' ')\n","    wid2pv.update({int(parts[0]): int(parts[1])})\n","    \n","# pv_rdd = sc.parallelize(wid2pv.items())\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(wid2pv, f)"]},{"cell_type":"markdown","id":"2a305711","metadata":{},"source":["**Build PageRank (Built once, used externally)**"]},{"cell_type":"code","execution_count":31,"id":"c354a98f","metadata":{},"outputs":[],"source":["# Calculate PageRank\n","pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.rdd.map(lambda x: (x.id, x.pagerank))\n","\n","pr_collected = pr.collect()\n","with open(\"PageRank_dict.pkl\", \"wb\") as f:\n","    pickle.dump(pr_collected, f)"]},{"cell_type":"markdown","id":"fd16c2ed","metadata":{},"source":["**Build Index**"]},{"cell_type":"code","execution_count":29,"id":"3ffabe01","metadata":{},"outputs":[],"source":["# Import pickle files to construct the index part-by-part\n","\n","# This part allows to take premade pkl files and load them to the work env in order to lighten\n","# the calculation and build the index\n","\n","\n","# pickle_file_path = '/home/dataproc/TermData.pkl'\n","\n","# # Load the pickle file\n","# with open(pickle_file_path, 'rb') as pickle_file:\n","#     TermData = pickle.load(pickle_file)\n","    \n","\n","# pickle_file_path = '/home/dataproc/W2DF.pkl'\n","\n","# # Load the pickle file\n","# with open(pickle_file_path, 'rb') as pickle_file:\n","#     w2df = pickle.load(pickle_file)\n","    \n","\n","# pickle_file_path = '/home/dataproc/Additional_Info.pkl'\n","\n","# # Load the pickle file\n","# with open(pickle_file_path, 'rb') as pickle_file:\n","#     additional_info_dict = pickle.load(pickle_file)\n","\n","# AVG_doc_size = 2.686965321606386 # Title\n","# AVG_doc_size = 365.655046299286  # Body\n","\n","# N = 6348910\n","\n","\n","# pickle_file_path = '/home/dataproc/Doc_Vectors_Title.pkl'\n","\n","# # Load the pickle file\n","# with open(pickle_file_path, 'rb') as pickle_file:\n","#     Doc_Vectors = pickle.load(pickle_file)\n","\n","\n","# pickle_file_path = '/home/dataproc/Posting_Locs_Body.pkl'\n","\n","# # Load the pickle file\n","# with open(pickle_file_path, 'rb') as pickle_file:\n","#     super_posting_locs = pickle.load(pickle_file)"]},{"cell_type":"code","execution_count":30,"id":"7d30b813","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating attributes\n","Creating posting\n","Creating docsize\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Avg doc size =  2.686965321606386\n","Writing PostingList to Bucket\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["INDEX_NAME_T = 'Index_Title_Final_Corrected'\n","BASE_DIR_T = 'Index_Title_Final_Corrected'\n","\n","# time the index creation time\n","t_start = time()\n","print('Creating attributes')\n","print('Creating posting')\n","# Preprocess, sort and filter doc_text_pairs for all indices and create the final posting list\n","word_counts = doc_text_pairs_title.flatMap(lambda x: ParseDocumentToPosting(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","postings = postings.filter(lambda x: len(x[1])>10)\n","\n","print('Creating docsize')\n","# build docsize - for index additional info\n","doc_sizes = doc_text_pairs_title.map(lambda x: ParseDocumentToDocSize(x[0], x[1]))\n","\n","# Get AVG doc size:\n","AVG_doc_size = (doc_sizes.map(lambda x: x[1]).reduce(lambda a, b: a + b)) / N\n","print('Avg doc size = ', AVG_doc_size)\n","\n","print('Creating term data')\n","# global statistics per term\n","TermData ,w2df = TermStats(postings,N)\n","\n","with open(\"TermData.pkl\", \"wb\") as f:\n","    pickle.dump(TermData, f)\n","with open(\"W2DF.pkl\", \"wb\") as f:\n","    pickle.dump(w2df, f)\n","\n","print('Creating doc data')\n","# Additional Info (Docs) Object\n","additional_info_dict = AdditionalInfoDict(doc_text_pairs_title, doc_sizes)\n","\n","with open(\"Additional_Info.pkl\", \"wb\") as f:\n","    pickle.dump(additional_info_dict, f)\n","\n","print('Creating doc vectors')\n","# Document TF-IDF vectors\n","Doc_Vectors = CalculateDocTFIDF_Vectors(postings, TermData, additional_info_dict)\n","with open(\"Doc_Vectors_Title.pkl\", \"wb\") as f:\n","    pickle.dump(Doc_Vectors, f)\n","\n","# partition posting lists and write out\n","# Title\n","print('Writing PostingList to Bucket')\n","\n","_ = partition_postings_and_write(postings, BASE_DIR_T).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=BASE_DIR_T):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","\n","with open(\"Posting_Locs_Title.pkl\", \"wb\") as f:\n","    pickle.dump(super_posting_locs, f)\n","\n","print('Building Index')\n","inverted = InvertedIndex()\n","\n","inverted.posting_locs = super_posting_locs\n","inverted.df = w2df\n","inverted.term_data = TermData\n","inverted.additional_info = additional_info_dict\n","inverted.doc_vectors = Doc_Vectors\n","inverted.N = N\n","inverted.AVG_doc_length = AVG_doc_size\n","\n","print('Writing Index')\n","# write the global stats out\n","inverted.write_index('.', INDEX_NAME_T)\n","# upload to gs\n","index_src = f\"{INDEX_NAME_T}.pkl\"\n","\n","index_dst = f'gs://{bucket_name}/{BASE_DIR_T}/{index_src}'\n","!gsutil cp $index_src $index_dst\n","print('Index creation Time (Title): ',(time() - t_start)/60, ' Minutes')"]},{"cell_type":"code","execution_count":34,"id":"77530a2e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building Index\n","Time so far:  4.53790028889974e-06  minutes\n","Writing Index\n","Copying file://Index_Body_Final_Corrected.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 18.8 MiB/ 18.8 MiB]                                                \n","Operation completed over 1 objects/18.8 MiB.                                     \n","Index creation Time (Body):  0.035965132713317874  Minutes\n"]}],"source":["INDEX_NAME_B = 'Index_Body_Final_Corrected'\n","BASE_DIR_B = 'Index_Body_Final_Corrected'\n","\n","# time the index creation time\n","t_start = time()\n","print('Creating attributes')\n","print('Creating posting')\n","# Preprocess, sort and filter doc_text_pairs for all indices and create the final posting list\n","word_counts = doc_text_pairs_body.flatMap(lambda x: ParseDocumentToPosting(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","postings = postings.filter(lambda x: len(x[1])>50)\n","\n","print('Creating docsize')\n","# build docsize - for index additional info\n","doc_sizes = doc_text_pairs_body.map(lambda x: ParseDocumentToDocSize(x[0], x[1]))\n","\n","# Get AVG doc size:\n","AVG_doc_size = (doc_sizes.map(lambda x: x[1]).reduce(lambda a, b: a + b)) / N\n","print(AVG_doc_size)\n","\n","# print('Time so far: ', (time() - t_start)/60, ' minutes')\n","print('Creating term data')\n","# global statistics per index\n","TermData ,w2df = TermStats(postings,N)\n","\n","with open(\"TermData.pkl\", \"wb\") as f:\n","    pickle.dump(TermData, f)\n","with open(\"W2DF.pkl\", \"wb\") as f:\n","    pickle.dump(w2df, f)\n","\n","# print('Time so far: ', (time() - t_start)/60, ' minutes')\n","print('Creating doc data')\n","# Additional Info (Docs) Object\n","additional_info_dict = AdditionalInfoDict(doc_text_pairs_title, doc_sizes)  # Will hold title, not body\n","\n","with open(\"Additional_Info.pkl\", \"wb\") as f:\n","    pickle.dump(additional_info_dict, f)\n","\n","partition posting lists and write out\n","# Body\n","print('Time so far: ', (time() - t_start)/60, ' minutes')\n","print('Writing PostingList to Bucket')\n","_ = partition_postings_and_write(postings, BASE_DIR_B).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=BASE_DIR_B):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","with open(\"Posting_Locs_Body.pkl\", \"wb\") as f:\n","    pickle.dump(super_posting_locs, f)\n","\n","print('Building Index')\n","inverted = InvertedIndex()\n","\n","inverted.posting_locs = super_posting_locs\n","inverted.df = w2df\n","inverted.additional_info = additional_info_dict\n","inverted.term_data = TermData\n","inverted.N = N\n","inverted.AVG_doc_length = AVG_doc_size\n","\n","print('Time so far: ', (time() - t_start)/60, ' minutes')\n","print('Writing Index')\n","# write the global stats out\n","inverted.write_index('.', INDEX_NAME_B)\n","# upload to gs\n","index_src = f\"{INDEX_NAME_B}.pkl\"\n","\n","index_dst = f'gs://{bucket_name}/{BASE_DIR_B}/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","print('Index creation Time (Body): ',(time() - t_start)/60, ' Minutes')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}